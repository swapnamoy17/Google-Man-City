{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#For Colab\nfrom google.colab import drive\ndrive.mount('/content/drive')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For Colab\nfrom tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown\nimport gdown\n\nurl = #Google drive link for saved model\noutput = 'qrdqn_6M_scenario_11.zip'\ngdown.download(url, output, quiet=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%%bash\n# dependencies\napt-get -y update > /dev/null\napt-get -y install libsdl2-gfx-dev libsdl2-ttf-dev > /dev/null\n\n# cloudpickle, pytorch, gym\npip3 install \"cloudpickle==1.3.0\"\npip3 install \"torch==1.5.1\"\npip3 install \"gym==0.17.2\"\n\n# gfootball\nGRF_VER=v2.8\nGRF_PATH=football/third_party/gfootball_engine/lib\nGRF_URL=https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_${GRF_VER}.so\ngit clone -b ${GRF_VER} https://github.com/google-research/football.git\nmkdir -p ${GRF_PATH}\nwget -q ${GRF_URL} -O ${GRF_PATH}/prebuilt_gameplayfootball.so\ncd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install . && cd ..\n\n# kaggle-environments\ngit clone https://github.com/Kaggle/kaggle-environments.git\ncd kaggle-environments && pip3 install . && cd ..\n\n# stable-baselines3\ngit clone https://github.com/DLR-RM/stable-baselines3.git\ncd stable-baselines3 && pip3 install . && cd ..\n\n# housekeeping\nrm -rf football kaggle-environments stable-baselines3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sb3-contrib","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom collections import OrderedDict\nimport base64\nimport pickle\nimport zlib\nimport gym\nimport numpy as np\nimport pandas as pd\nimport torch as th\nfrom torch import nn, tensor\nfrom collections import deque\nfrom gym.spaces import Box, Discrete\nfrom kaggle_environments import make\nfrom kaggle_environments.envs.football.helpers import *\nfrom gfootball.env import create_environment, observation_preprocessing\nfrom sb3_contrib import QRDQN\nfrom sb3_contrib.qrdqn import CnnPolicy\nfrom stable_baselines3.common import results_plotter\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.env_checker import check_env\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\nfrom stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\nfrom stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\nfrom stable_baselines3.common.vec_env import VecTransposeImage\nfrom IPython.display import HTML\nfrom matplotlib import pyplot as plt\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Google-Football\nclass FootballGym(gym.Env):\n    spec = None\n    metadata = None\n    \n    def __init__(self, config=None):\n        super(FootballGym, self).__init__()\n        env_name = \"11_vs_11_easy_stochastic\"\n        rewards = \"scoring,checkpoints\"\n        if config is not None:\n            env_name = config.get(\"env_name\", env_name)\n            rewards = config.get(\"rewards\", rewards)\n        self.env = create_environment(\n            env_name=env_name,\n            stacked=False,\n            representation=\"raw\",\n            rewards = rewards,\n            write_goal_dumps=False,\n            write_full_episode_dumps=False,\n            render=False,\n            write_video=False,\n            dump_frequency=1,\n            logdir=\".\",\n            extra_players=None,\n            number_of_left_players_agent_controls=1,\n            number_of_right_players_agent_controls=0)  \n        self.action_space = Discrete(19)\n        self.observation_space = Box(low=0, high=255, shape=(72, 96, 16), dtype=np.uint8)\n        self.reward_range = (-1, 1)\n        self.obs_stack = deque([], maxlen=4)\n        \n    def transform_obs(self, raw_obs):\n        obs = raw_obs[0]\n        obs = observation_preprocessing.generate_smm([obs])\n        if not self.obs_stack:\n            self.obs_stack.extend([obs] * 4)\n        else:\n            self.obs_stack.append(obs)\n        obs = np.concatenate(list(self.obs_stack), axis=-1)\n        obs = np.squeeze(obs)\n        return obs\n\n    def reset(self):\n        self.obs_stack.clear()\n        obs = self.env.reset()\n        obs = self.transform_obs(obs)\n        return obs\n    \n    def step(self, action):\n        obs, reward, done, info = self.env.step([action])\n        obs = self.transform_obs(obs)\n        return obs, float(reward), done, info\n    \ncheck_env(env=FootballGym(), warn=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CNN model\nclass FootballCNN(BaseFeaturesExtractor):\n    def __init__(self,observation_space,features_dim = 256):\n        super(FootballCNN,self).__init__(observation_space, features_dim)\n        in_channels = observation_space.shape[0] # channels x height x width\n        self.cnn = nn.Sequential(OrderedDict([('conv1_depthwise', nn.Conv2d(16,16,3, stride=2, padding=1, groups=16)),\n                        ('conv1_pointwise', nn.Conv2d(16,32,1)),\n                        ('Relu1', nn.ReLU()),\n                        ('Pooling layer',nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, ceil_mode=False)),\n                        ('conv2_depthwise', nn.Conv2d(32,32,3, stride=2, padding=1, groups=32)),\n                        ('conv2_pointwise', nn.Conv2d(32,64,1)),\n                        ('Relu2', nn.ReLU()),\n                        ('conv3_depthwise', nn.Conv2d(64,64,3, stride=2, padding=1, groups=64)),\n                        ('conv3_pointwise', nn.Conv2d(64,128,1)),\n                        ('Relu3', nn.ReLU()),\n                        ('Flatten', nn.Flatten())]))\n        self.linear = nn.Sequential(OrderedDict([('Linear',nn.Linear(in_features=3840, out_features=features_dim, bias=True)),\n                          ('Relu4', nn.ReLU())]))\n          \n\n    def forward(self, obs):\n        return self.linear(self.cnn(obs))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scenarios = {0: \"academy_empty_goal_close\",            # academy_difficulty = 0.6\n             1: \"academy_empty_goal\",\n             2: \"academy_run_to_score\",\n             3: \"academy_run_to_score_with_keeper\",\n             4: \"academy_pass_and_shoot_with_keeper\",\n             5: \"academy_run_pass_and_shoot_with_keeper\",\n             6: \"academy_3_vs_1_with_keeper\",\n             7: \"academy_corner\",\n             8: \"academy_counterattack_easy\",\n             9: \"academy_counterattack_hard\",\n             10: \"academy_single_goal_versus_lazy\",\n             11: \"11_vs_11_easy_stochastic\",         #difficulty: 0.05\n             12: \"11_vs_11_stochastic\",              #difficulty: 0.6\n             13: \"11_vs_11_hard_stochastic\",         #difficulty: 0.95 \n             14: \"11_vs_11_kaggle\"}                  #difficulty: 1\n\nscenario_index = 11\nif scenario_index >=0 and scenario_index <=9:\n  scenario_length = 401\nelse:\n  scenario_length = 3001\n  \nscenario_name = scenarios[scenario_index]\nprint(scenario_name)\nrewards = \"scoring\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_env(config=None, rank=0):\n    def _init():\n        env = FootballGym(config)\n        log_file = os.path.join(\".\", str(rank))\n        env = Monitor(env, log_file, allow_early_resets=True)\n        return env\n    return _init\n\nn_envs = 1\nn_steps = 512\n#config={\"env_name\":scenario_name}\n#train_env = DummyVecEnv([make_env(config, rank=i) for i in range(n_envs)])\n#eval_env = DummyVecEnv([make_env(config, rank=i) for i in range(1)])\ntrain_env = DummyVecEnv([make_env({\"env_name\":scenario_name, \"rewards\": rewards})])\neval_env = VecTransposeImage(DummyVecEnv([make_env({\"env_name\":scenario_name, \"rewards\": \"scoring,checkpoints\"})]))\n#train_env = SubprocVecEnv([make_env(config, rank=i)])\n#eval_env = SubprocVecEnv([make_env(config, rank=i)])\n\npolicy_kwargs = dict(features_extractor_class=FootballCNN,\n                     features_extractor_kwargs=dict(features_dim=256),\n                     n_quantiles=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Starting training for the first time\nmodel = QRDQN(CnnPolicy, train_env, \n            policy_kwargs=policy_kwargs,learning_rate=1e-5, \n            buffer_size=10000, learning_starts=16, \n            batch_size=32, \n            tau= 1, gamma=0.99, \n            train_freq=4, gradient_steps=1, \n            optimize_memory_usage=False, \n            target_update_interval=2500, \n            exploration_fraction=0.01, \n            exploration_initial_eps=1.0, \n            exploration_final_eps=0.01, \n            max_grad_norm=0.5,\n            verbose=1,  \n            tensorboard_log =\"./tensorboard/\",\n            seed=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Continue training\nobs=train_env.reset()\nmodel = QRDQN.load(\"../input/qrdqn-xception/qrdqn_2750000_steps.zip\",env=train_env,verbose=1)\n#model.set_random_seed(seed=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n\neval_freq = scenario_length*20\neval_callback = EvalCallback(eval_env=eval_env, best_model_save_path='./models/',\n                             log_path='./logs/', eval_freq=eval_freq, n_eval_episodes = 5,\n                             deterministic=True, render=False, verbose=1)\n\ncheckpoint_callback = CheckpointCallback(save_freq=250000, save_path='./',\n                                         name_prefix='qrdqn')\n\ntotal_timesteps = scenario_length*15000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.learn(total_timesteps=total_timesteps, callback=[eval_callback,checkpoint_callback], reset_num_timesteps=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"./qrdqn_3620192_steps\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Downloading Checkpoints","metadata":{}},{"cell_type":"code","source":"#For Kaggle\n!tar -zcvf qrdqn_scenario_3.tar.gz ./\n\nfrom IPython.display import FileLink\nFileLink(r'qrdqn_scenario_3.tar.gz')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation Policy (After Training)","metadata":{}},{"cell_type":"code","source":"#If we are loading the model from previous runs and running evaluation code\nmodel = QRDQN.load(\"./qrdqn_6M_scenario_11.zip\",env=eval_env,verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from stable_baselines3.common.evaluation import evaluate_policy\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=5, deterministic=True)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=5, deterministic=False)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nlogs = np.load('logs/evaluations.npz')\n#episodes = np.transpose(logs['timesteps']/3001)\nepisodes = (logs['timesteps']/3001).tolist()\nrewards = (logs['results'].T[0]).tolist()\ndata = np.array([episodes,rewards]).T\nfig = plt.figure()\n#ax1 = fig.add_subplot(121)\n#fig.set_title(\"scores\")\nfig.suptitle('Validation scores')\nscores = pd.DataFrame(data=data, columns= [\"Episodes\", \"Scores\"])\nsns.lineplot(x=\"Episodes\", y=\"Scores\", data=scores)\nplt.show()\nplt.savefig('validation.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Writing Subsmission File (All of this is written in subsmission.py)**","metadata":{}},{"cell_type":"code","source":"%%writefile submission.py\nimport base64\nimport pickle\nimport zlib\nimport numpy as np\nimport torch as th\nfrom torch import nn, tensor\nfrom collections import deque\nfrom gfootball.env import observation_preprocessing\nfrom collections import OrderedDict\n\nstate_dict = _STATE_DICT_\n\nstate_dict = pickle.loads(zlib.decompress(base64.b64decode(state_dict)))\n    \nclass PyTorchCnnPolicy(nn.Module):\n    global state_dict\n    def __init__(self):\n        super().__init__()\n        self.cnn = nn.Sequential(OrderedDict([('conv1_depthwise', nn.Conv2d(16,16,3, stride=2, padding=1, groups=16)),\n                        ('conv1_pointwise', nn.Conv2d(16,32,1)),\n                        ('Relu1', nn.ReLU()),\n                        ('Pooling layer',nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, ceil_mode=False)),\n                        ('conv2_depthwise', nn.Conv2d(32,32,3, stride=2, padding=1, groups=32)),\n                        ('conv2_pointwise', nn.Conv2d(32,64,1)),\n                        ('Relu2', nn.ReLU()),\n                        ('conv3_depthwise', nn.Conv2d(64,64,3, stride=2, padding=1, groups=64)),\n                        ('conv3_pointwise', nn.Conv2d(64,128,1)),\n                        ('Relu3', nn.ReLU()),\n                        ('Flatten', nn.Flatten())]))\n        self.linear = nn.Sequential(OrderedDict([('Linear',nn.Linear(in_features=3840, out_features=256, bias=True)),\n                          ('Relu4', nn.ReLU())]))\n\n        self.quantile_net = nn.Sequential(\n          nn.Linear(in_features=256, out_features=64, bias=True),\n          nn.ReLU(),\n          nn.Linear(in_features=64, out_features=64, bias=True),\n          nn.ReLU(),\n          nn.Linear(in_features=64, out_features=3800, bias=True),\n          nn.ReLU(),\n        )\n        self.out_activ = nn.Softmax(dim=1)\n        self.load_state_dict(state_dict)\n\n    def forward(self, x):\n        x = tensor(x).float() / 255.0  # normalize\n        x = x.permute(0, 3, 1, 2).contiguous()  # 1 x channels x height x width\n        x = self.cnn(x)\n        x = self.linear(x)\n        x = self.quantile_net(x)\n        x = self.out_activ(x)\n        return int(x.argmax())\n    \n    def predict(self, observation: th.Tensor, deterministic: bool = True) -> th.Tensor:\n        q_values = self.forward(observation).mean(dim=1)\n        # Greedy action\n        action = q_values.argmax(dim=1).reshape(-1)\n        return action\n    \nobs_stack = deque([], maxlen=4)\ndef transform_obs(raw_obs):\n    global obs_stack\n    obs = raw_obs['players_raw'][0]\n    obs = observation_preprocessing.generate_smm([obs])\n    if not obs_stack:\n        obs_stack.extend([obs] * 4)\n    else:\n        obs_stack.append(obs)\n    obs = np.concatenate(list(obs_stack), axis=-1)\n    return obs\n\npolicy = PyTorchCnnPolicy()\npolicy = policy.float().to('cpu').eval()\n\n# def agent(raw_obs):\n#     obs = transform_obs(raw_obs)\n#     #action = policy(obs)\n#     action = policy.predict(obs)\n#     return action\n\ndef agent(observation: th.Tensor, deterministic: bool = True) -> th.Tensor:\n        observation = transform_obs(observation)\n        print('obs:',observation)\n        q_values = policy.forward(observation).mean(dim=1)\n        # Greedy action\n        action = q_values.argmax(dim=1).reshape(-1)\n        return action\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**To get the name of the layers in dict that are to be written in subs.py**","metadata":{}},{"cell_type":"code","source":"model = QRDQN.load(\"qrdqn_6M_scenario_11.zip\")\n_state_dict = model.policy.to('cpu').state_dict()\ndic={}\nfor param_tensor in _state_dict:\n    print(param_tensor, \"\\t\", _state_dict[param_tensor].size())\n    key = param_tensor.split('.')\n    if key[1]=='quantile_net':\n        key = '.'.join(key[1:])\n    else:\n        key = '.'.join(key[2:])\n    #print(\"key=\",key)\n    dic[key]=_state_dict[param_tensor]\n    \nstate_dict = dic\n# state_dict = {\n#     \"cnn.conv1_depthwise.weight\":_state_dict['quantile_net.features_extractor.cnn.conv1_depthwise.weight'], \n#     \"cnn.conv1_depthwise.bias\":_state_dict['quantile_net.features_extractor.cnn.conv1_depthwise.bias'], \n#     \"cnn.conv1_pointwise.weight\":_state_dict['quantile_net.features_extractor.cnn.conv1_pointwise.weight'], \n#     \"cnn.conv1_pointwise.bias\":_state_dict['quantile_net.features_extractor.cnn.conv1_pointwise.bias'],\n#     \"cnn.conv2_depthwise.weight\":_state_dict['quantile_net.features_extractor.cnn.conv2_depthwise.weight'], \n#     \"cnn.conv2_depthwise.bias\":_state_dict['quantile_net.features_extractor.cnn.conv2_depthwise.bias'], \n#     \"cnn.conv2_pointwise.weight\":_state_dict['quantile_net.features_extractor.cnn.conv2_pointwise.weight'], \n#     \"cnn.conv2_pointwise.bias\":_state_dict['quantile_net.features_extractor.cnn.conv2_pointwise.bias'],\n#     \"cnn.conv3_depthwise.weight\":_state_dict['quantile_net.features_extractor.cnn.conv3_depthwise.weight'], \n#     \"cnn.conv3_depthwise.bias\":_state_dict['quantile_net.features_extractor.cnn.conv3_depthwise.bias'], \n#     \"cnn.conv3_pointwise.weight\":_state_dict['quantile_net.features_extractor.cnn.conv3_pointwise.weight'], \n#     \"cnn.conv3_pointwise.bias\":_state_dict['quantile_net.features_extractor.cnn.conv3_pointwise.bias'],\n#     \"linear.Linear.weight\":_state_dict['quantile_net.features_extractor.linear.Linear.weight'], \n#     \"linear.Linear.bias\":_state_dict['quantile_net.features_extractor.linear.Linear.bias'], \n#     \"action_net.0.weight\":_state_dict['quantile_net.quantile_net.0.weight'],\n#     \"action_net.0.bias\":_state_dict['quantile_net.quantile_net.0.bias'],\n# }\n\nstate_dict = base64.b64encode(zlib.compress(pickle.dumps(state_dict)))\nwith open('submission.py', 'r') as file:\n    src = file.read()\nsrc = src.replace(\"_STATE_DICT_\", f\"{state_dict}\")\nwith open('submission.py', 'w') as file:\n    file.write(src)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import make\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": scenario_name, \"running_in_notebook\": True}, debug=True)\nagent = \"submission.py\"\noutput = env.run([agent, \"run_right\"])[-1]\nprint('Left player: action = %s, reward = %s, status = %s, info = %s' % (output[0][\"action\"], output[0]['reward'], output[0]['status'], output[0]['info']))\nprint('Right player: action = %s, reward = %s, status = %s, info = %s' % (output[1][\"action\"], output[1]['reward'], output[1]['status'], output[1]['info']))\nenv.render(mode=\"human\", width=800, height=600)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}